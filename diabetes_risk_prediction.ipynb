{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing and Exploration\n",
        "# NumPy for numerical operations used during preprocessing\n",
        "import numpy as np\n",
        "\n",
        "# Pandas for loading the diabetes dataset and preparing it for modeling\n",
        "import pandas as pd\n",
        "\n",
        "# Decision Tree classifier, one of the models used to predict diabetes classes\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Matplotlib Pyplot for plotting distributions and model visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics used to evaluate how well each model predicts diabetes categories\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# plot_tree for displaying the structure of the trained decision tree model\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Tools for splitting the dataset into training/testing sets and optimizing model performance\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "\n",
        "# KNN classifier, another model used to classify patient diabetes status based on similarity\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Pipeline to connect preprocessing steps (scaling) with the model in one workflow\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Scalers used to normalize or standardize medical measurements before training the models\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# SVM classifier, one of the main models selected for multi-class diabetes prediction\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Seaborn for creating advanced visualizations during the exploratory data analysis phase\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "t0qJa1U1nlnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "QTBbasoQpAjN",
        "outputId": "ba74749f-5990-46e7-8529-a37c6943847a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Dataset of Diabetes .csv')\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "id": "W4xHkaHzAiY0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "cBRipShejZx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "eeUarfokjaOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a full summary of the dataset to review each column’s data type, non-null count, and overall structure\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "CtbKUrv1rGTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce descriptive statistics for all numerical features to understand their distributions, ranges, and central tendencies\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "8e-8QhUqr_tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of unique values in each column of the dataset\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "m_pZssrFsCHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing data by counting how many NaN values appear in each column of the dataset\n",
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "Z9YCy3uAsKTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of the diabetes outcome classes to check for imbalance in the target variable\n",
        "print('\\nClass distribution:')\n",
        "print(dataset['CLASS'].value_counts())\n",
        "\n",
        "# The repeated class labels indicate inconsistent formatting in the dataset"
      ],
      "metadata": {
        "id": "EJtmhKaWsQy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up target labels\n",
        "dataset['CLASS'] = dataset['CLASS'].astype(str).str.strip().str.upper()"
      ],
      "metadata": {
        "id": "vWbmPLMTsXK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display class distribution\n",
        "print('\\nClass distribution:')\n",
        "print(dataset['CLASS'].value_counts())"
      ],
      "metadata": {
        "id": "Qv2kz0XushP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display class distribution\n",
        "print('\\nClass distribution:')\n",
        "print(dataset['Gender'].value_counts())"
      ],
      "metadata": {
        "id": "8y_qcq5OsoyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up target labels\n",
        "dataset['Gender'] = dataset['Gender'].astype(str).str.strip().str.upper()"
      ],
      "metadata": {
        "id": "pUSmOe2Ysvoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up target labels\n",
        "dataset['Gender'] = dataset['Gender'].astype(str).str.strip().str.upper()"
      ],
      "metadata": {
        "id": "qGSdR2dGEJD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize age distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(dataset['AGE'], bins=20, kde=True, color='orange')\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uwSEVe_gs8fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(x='CLASS', hue='Gender', data=dataset)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QQKQFfBAtFE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(dataset)"
      ],
      "metadata": {
        "id": "ZH_KvObmtiqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(2)"
      ],
      "metadata": {
        "id": "3yoh9_m-trFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify all categorical (object-type) columns in the dataset for encoding\n",
        "cat_columns = dataset.select_dtypes(['object']).columns\n",
        "\n",
        "# Convert each categorical column into numeric codes using factorization\n",
        "dataset[cat_columns] = dataset[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
        "\n",
        "# Display the first two rows to confirm that categorical features were successfully encoded\n",
        "dataset.head(2)"
      ],
      "metadata": {
        "id": "1LZgr-Ixts9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the feature matrix (X) by removing the target label (CLASS) and non-predictive identifiers (ID, No_Pation)\n",
        "X = dataset.drop(columns=['CLASS', 'ID', 'No_Pation'])\n",
        "\n",
        "# Extract the diabetes classification labels into the target vector (y)\n",
        "y = dataset['CLASS']\n",
        "\n",
        "# Display the first few rows of the feature matrix\n",
        "X.head()"
      ],
      "metadata": {
        "id": "gtskStXft_V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply standardization to the feature set so all medical measurements share a comparable scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Preview the standardized feature matrix\n",
        "X_scaled.head(2)"
      ],
      "metadata": {
        "id": "1siPGOgjunlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First split: divide the dataset into an 80% training+validation set and a 20% test set,\n",
        "# ensuring class proportions are preserved with stratified sampling\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Second split: separate the training+validation set into 60% training and 20% validation,\n",
        "# maintaining overall class balance across all subsets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "# Display the combined training and validation feature set\n",
        "X_train_val"
      ],
      "metadata": {
        "id": "-xferkPKuv7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the combined train+val set since GridSearchCV handles its own validation internally\n",
        "X_grid = X_train_val\n",
        "y_grid = y_train_val\n",
        "\n",
        "# Create a pipeline that first normalizes the data, then applies SVM\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),    # Apply feature scaling\n",
        "    ('svm', SVC())                   # SVM model\n",
        "])\n",
        "\n",
        "# Parameter options that will be tested during the grid search\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],           # Regularization values to try\n",
        "    'svm__kernel': ['linear', 'rbf'], # Kernel types to compare\n",
        "    'svm__gamma': ['scale', 'auto']   # Gamma settings for RBF kernel\n",
        "}\n",
        "\n",
        "# Perform the grid search with 5-fold CV to find the best setup\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the search on the training+validation portion\n",
        "grid_search.fit(X_grid, y_grid)\n",
        "\n",
        "# Display the best settings found and their CV accuracy\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Test the optimized model on the separate test split\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Show test accuracy plus detailed class results\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "JMobbdj3Q5Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix and performance metrics\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "3LV1wzKdQ-Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', linewidths=0.5, linecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i5gvOUxoR7w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Decision Tree using Entropy\n",
        "# Build a Decision Tree with simple depth and leaf settings to reduce overfitting\n",
        "clf = DecisionTreeClassifier(criterion = \"entropy\", random_state = 42,max_depth = 3, min_samples_leaf = 5)\n",
        "# Fit the model on the training data\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ou5gdRUZikLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Decision Tree on the Test Set\n",
        "# Predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy and classification metrics\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "snKFDhTzUCkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Confusion Matrix\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', linewidths=0.5, linecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E33hxOggVOcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into Train, Validation, and Test Sets\n",
        "\n",
        "# Standard split: 80% train, 10% validation, 10% test\n",
        "# (Split already done earlier – here we only display the sizes\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Validation set size: {len(X_val)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "\n",
        "#Tune Hyperparameters to Reduce Overfitting\n",
        "\n",
        "# Overfitting occurs when the model learns the training data too well, including its noise.\n",
        "# Use GridSearchCV to try different parameters and pick the best one\n",
        "\n",
        "# Key parameters for preventing overfitting in Decision Trees:\n",
        "#   - max_depth: Limits how deep the tree can grow. A smaller value reduces complexity.\n",
        "#   - min_samples_leaf: Minimum number of samples required to be at a leaf node.\n",
        "#   - ccp_alpha: Cost-complexity pruning parameter. A higher value leads to more pruning.\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_leaf': [1, 5, 10, 20],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Base Decision Tree estimator\n",
        "dt_base = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV using 5-fold cross validation\n",
        "grid_search = GridSearchCV(estimator=dt_base, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Print the best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"\\nBest hyperparameters found via GridSearchCV: {best_params}\")\n",
        "\n",
        "\n",
        "# Train Final Decision Tree with Best Parameters\n",
        "best_dt_model = DecisionTreeClassifier(\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    criterion=best_params['criterion'],\n",
        "    random_state=42)\n",
        "\n",
        "best_dt_model.fit(X_train_val, y_train_val)\n",
        "\n",
        "\n",
        "# Evaluate the Optimized Model\n",
        "# Test the tuned model on unseen data.\n",
        "y_pred = best_dt_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy of the best model on the test set: {accuracy:.2f}\")\n",
        "\n",
        "# Visualize the Optimized Decision Tree\n",
        "# Display the pruned tree structure with colors and labels.\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(best_dt_model,\n",
        "          feature_names=X.columns,\n",
        "          class_names=['Diabetic', 'Non-Diabetic', 'Predict-Diabetic'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=8,\n",
        "          impurity=False)\n",
        "plt.title(\"Optimized Decision Tree for Diabetes Classification\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0cEkQZt8Vl3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train K-Nearest Neighbors Model\n",
        "# Use KNN with k = 9 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "knn.fit(X_train_val, y_train_val)"
      ],
      "metadata": {
        "id": "-LvMCl0oVytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate KNN on the Test Set\n",
        "# Predictions on the test set\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Confusion matrix and performance metrics\n",
        "cm = confusion_matrix(y_test, y_pred_knn)\n",
        "accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "report = classification_report(y_test, y_pred_knn)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "tZLzi_1XWEQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize KNN Confusion Matrix\n",
        "\n",
        "# predictions on the test data\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_knn)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Reds\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NJ_7CeteWLyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}